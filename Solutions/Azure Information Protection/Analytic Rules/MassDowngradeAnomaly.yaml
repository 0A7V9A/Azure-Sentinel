id: fc1b7e7a-bc24-42c3-ad67-5c76c8fcb2d6
name: (Preview) Unusual mass downgrade AIP label
kind: Anomaly
description: |
  This algorithm detects unusual high volume of downgrade label activity in Azure Information Protection (AIP) logs.
  It considers "AIP" workload records for a given number of days and determines the sequence of activity performed on documents along with the label applied to classify unusual volume of downgrade activity.
anomalyDefinitionVersion: 1.0.3
owner: arellave@microsoft.com
packageLocation: /anomalies/fc1b7e7a-bc24-42c3-ad67-5c76c8fcb2d6/1.0.3/massDowngradeAnomaly-1.0.3-py3-none-any.whl
customizableObservations:
  prioritizeExcludeObservations:
  - prioritize:
    name: File extension
    description: 'Give comma separated file extension to exclude from source data within double quotes, example: ".txt,.jpg,.mp4"'
    rerun: RerunAlways
    dataType: string
    sequenceNumber: 2
    exclude: .txt,.jpg,.mp4
  thresholdObservations:
  - minimum: '5'
    maximum: '100'
    value: '10'
    name: Minimum number of downgrades
    description: Generate an anomaly when number of downgrade label is greater than the chosen value
    sequenceNumber: 1
    rerun: RerunAlways
requiredDataConnectors:
- connectorId: AzureInformationProtection
  dataTypes:
  - InformationProtectionLogs_CL
jobArguments:
  workspaceId: 00000000-0000-0000-0000-000000000000
frequency: PT24H
environment: Production
tactics:
- Collection
techniques:
- T1530
- T1213
- T1005
- T1039
- T1114
severity: Informational
status: Available
sparkParameters:
  spark.databricks.io.cache.maxMetaDataCache: 10g
  spark.databricks.io.cache.enabled: true
  spark.databricks.io.cache.maxDiskUsage: 20g
  spark.sql.execution.arrow.enabled: true
  spark.sql.streaming.stopActiveRunOnRestart: true
  spark.databricks.delta.preview.enabled: true
  spark.databricks.io.cache.compression.enabled: true
  fs.azure.enable.append.support: true
  fs.azure.rename.threads: 10
  fs.azure.delete.threads: 10
  spark.hadoop.validateOutputSpecs: true
  spark.sql.hive.metastorePartitionPruning: true
  spark.cleaner.periodicGC.interval: 5min
  spark.cleaner.referenceTracking.blocking.shuffle: false
  spark.cleaner.referenceTracking.blocking: false
  spark.cleaner.referenceTracking.cleanCheckpoints: true
  spark.databricks.delta.snapshotPartitions: 1000
createdDateUtc: 2020-11-18
lastUpdatedDateUTC: 2021-06-04
