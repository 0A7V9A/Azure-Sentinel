id: 543c9254-eb6f-4fdd-858d-783e0e3d5cb9
name: (Preview) Suspicious number of protected documents accessed
kind: Anomaly
description: "This algorithm is to detect high volume of access to protected documents in Azure Information Protection (AIP) logs. \nIt considers AIP workload records for a given number of days and determines whether the user performed unusual access to protected documents in a day given his/her historical behavior.\n"
anomalyDefinitionVersion: 1.0.4
owner: arellave@microsoft.com
packageLocation: /anomalies/543c9254-eb6f-4fdd-858d-783e0e3d5cb9/1.0.4/suspiciousProtectedDocAccess-1.0.4-py3-none-any.whl
customizableObservations:
  prioritizeExcludeObservations:
  - prioritize:
    name: File extension
    description: 'Give comma separated file extension to exclude from source data, for example: .txt,.jpg,.mp4'
    rerun: RerunAlways
    dataType: string
    sequenceNumber: 2
    exclude: .msg,.jpg,.txt
  thresholdObservations:
  - name: Score
    minimum: '0'
    maximum: '1'
    value: '0.9'
    description: Generate an anomaly when the error percentile is greater than chosen value
    sequenceNumber: 1
    rerun: NotRequired
requiredDataConnectors:
- connectorId: AzureInformationProtection
  dataTypes:
  - InformationProtectionLogs_CL
jobArguments:
  workspaceId: 00000000-0000-0000-0000-000000000000
frequency: PT24H
environment: Production
tactics:
- Collection
techniques:
- T1530
- T1213
- T1005
- T1039
- T1114
severity: Informational
status: Available
sparkParameters:
  spark.databricks.io.cache.maxMetaDataCache: 10g
  spark.databricks.io.cache.enabled: true
  spark.databricks.io.cache.maxDiskUsage: 20g
  spark.sql.execution.arrow.enabled: true
  spark.sql.streaming.stopActiveRunOnRestart: true
  spark.databricks.delta.preview.enabled: true
  spark.databricks.io.cache.compression.enabled: true
  fs.azure.enable.append.support: true
  fs.azure.rename.threads: 10
  fs.azure.delete.threads: 10
  spark.hadoop.validateOutputSpecs: true
  spark.sql.hive.metastorePartitionPruning: true
  spark.cleaner.periodicGC.interval: 5min
  spark.cleaner.referenceTracking.blocking.shuffle: false
  spark.cleaner.referenceTracking.blocking: false
  spark.cleaner.referenceTracking.cleanCheckpoints: true
  spark.databricks.delta.snapshotPartitions: 1000
createdDateUtc: 2021-02-24
lastUpdatedDateUTC: 2022-05-09
